{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ccd9c638-9ffa-4fb5-9d65-399d787d18da",
      "metadata": {
        "id": "ccd9c638-9ffa-4fb5-9d65-399d787d18da"
      },
      "source": [
        "# Manifesto data retrieval\n",
        "For US, Japan"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1db3299-43ae-4a8b-be7e-d90dcca9f171",
      "metadata": {
        "id": "b1db3299-43ae-4a8b-be7e-d90dcca9f171"
      },
      "source": [
        "## Download Manifesto Data\n",
        "- https://manifesto-project.wzb.eu/information/documents/api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Google Colab\n",
        "!pip install dotenv langdetect deep_translator googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00LTWvPaTNIi",
        "outputId": "3108759a-2770-4906-9c95-b205e480692d"
      },
      "id": "00LTWvPaTNIi",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deep_translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.4.26)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting python-dotenv (from dotenv)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.4.0)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans, langdetect\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=00eb7c7876d258a65166dda0d97736c6c496cf25a0d982ec5cf057fee10e6f00\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=8c3df536803f6995c62f6eaf883d27410405408d73c0cd6e001f722f2d5fb39f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built googletrans langdetect\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, python-dotenv, langdetect, idna, hstspreload, h2, httpcore, dotenv, httpx, deep_translator, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.16.0\n",
            "    Uninstalling h11-0.16.0:\n",
            "      Successfully uninstalled h11-0.16.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.9\n",
            "    Uninstalling httpcore-1.0.9:\n",
            "      Successfully uninstalled httpcore-1.0.9\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.20.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.86.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "langsmith 0.3.45 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "gradio 5.31.0 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
            "gradio-client 1.10.1 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 deep_translator-1.11.4 dotenv-0.9.9 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 langdetect-1.0.9 python-dotenv-1.1.0 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "db90864b-5c7f-4c51-87e2-b8842efd4998",
      "metadata": {
        "id": "db90864b-5c7f-4c51-87e2-b8842efd4998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "79170039-e2cc-469a-aaad-18b006235a18"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3633246412>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_manifesto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDownloadManifesto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdotenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dotenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import dotenv\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "\n",
        "sys.path.append(\"../\")\n",
        "from src.data.download_manifesto import DownloadManifesto\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "dataset_key = \"MPDS2024a\"\n",
        "version = '2024-1'\n",
        "api_key = os.getenv(\"MANIFESTO_API\")\n",
        "downloader =  DownloadManifesto(dataset_key, version, api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a577517-1918-4d42-a807-09d70d71545c",
      "metadata": {
        "id": "5a577517-1918-4d42-a807-09d70d71545c"
      },
      "outputs": [],
      "source": [
        "countries = ['United States', 'Japan']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "600f6b11-a429-4ebb-85e0-9d95e621c36e",
      "metadata": {
        "id": "600f6b11-a429-4ebb-85e0-9d95e621c36e"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "df_all_countries = []\n",
        "for country in tqdm(countries):\n",
        "    print(country)\n",
        "    result = downloader.get_country_data(country)\n",
        "    df, metadata = downloader.get_metadata(result)\n",
        "    df_country = downloader.get_texts(df)  # Get texts\n",
        "    df_all_countries.append(df_country)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfa0c2a2-08a4-4b5a-918b-d9eff00cbbaa",
      "metadata": {
        "id": "dfa0c2a2-08a4-4b5a-918b-d9eff00cbbaa"
      },
      "outputs": [],
      "source": [
        "df_all = pd.concat(df_all_countries, axis='rows')\n",
        "df_all = df_all.rename(lambda x: pd.to_datetime(x, format = \"%Y%m\"), axis=0, level=1) # convert date to date time\n",
        "df_all = df_all.reset_index()\n",
        "df_all.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "880e8c63-b3fb-4b29-b377-57ccb30ab555",
      "metadata": {
        "id": "880e8c63-b3fb-4b29-b377-57ccb30ab555"
      },
      "outputs": [],
      "source": [
        "file_name = \"../data/processed/manifesto_us_japan.parquet\"\n",
        "df_all.to_parquet(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf2a1b43",
      "metadata": {
        "id": "bf2a1b43"
      },
      "source": [
        "## Import generated data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ccb47bf2",
      "metadata": {
        "id": "ccb47bf2"
      },
      "outputs": [],
      "source": [
        "df_all = pd.read_parquet(\"../data/processed/manifesto_us_japan.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89efbbe8-af98-48d4-9d8c-921504175117",
      "metadata": {
        "id": "89efbbe8-af98-48d4-9d8c-921504175117"
      },
      "source": [
        "## Count words across time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9afea11-2a38-409e-9644-97e80d742c45",
      "metadata": {
        "id": "b9afea11-2a38-409e-9644-97e80d742c45"
      },
      "source": [
        "### Count by country (only for English speaking countries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "079db915-6d63-4471-ad07-99e38004c034",
      "metadata": {
        "id": "079db915-6d63-4471-ad07-99e38004c034",
        "outputId": "6661ef53-6a27-4849-9385-6c4ba47e040e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ghg': 0, 'greenhouse': 23, 'net-zero': 0, 'carbon': 42}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vocab_list = ['ghg', 'greenhouse', 'net-zero', 'carbon']\n",
        "vectorizer = CountVectorizer(\n",
        "    stop_words = 'english',     # Remove stop words. Can be a list of stop words or a string from {'english', 'spanish'}.\n",
        "    lowercase = True,           # Convert text to lowercase.\n",
        "    ngram_range = (1,1),\n",
        "    vocabulary = vocab_list\n",
        "\n",
        ")\n",
        "counts = vectorizer.fit_transform(df_all['text']).toarray().sum(axis=0)\n",
        "word_freq = dict(zip(vectorizer.get_feature_names_out(), counts))\n",
        "word_freq"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de453867-1c3e-4247-90cc-fd7474dd789a",
      "metadata": {
        "id": "de453867-1c3e-4247-90cc-fd7474dd789a"
      },
      "source": [
        "### Count by year and date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd49c97-96db-4adc-b040-afa14872dc99",
      "metadata": {
        "id": "0bd49c97-96db-4adc-b040-afa14872dc99",
        "outputId": "a57e7c9f-0b04-48a5-bae3-149cf4dc2d03"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>countryname</th>\n",
              "      <th>date</th>\n",
              "      <th>party</th>\n",
              "      <th>partyname</th>\n",
              "      <th>keys</th>\n",
              "      <th>manifesto_id</th>\n",
              "      <th>text</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>United States</td>\n",
              "      <td>1960-11-01</td>\n",
              "      <td>61320</td>\n",
              "      <td>Democratic Party</td>\n",
              "      <td>61320_196011</td>\n",
              "      <td>61320_196011</td>\n",
              "      <td>In 1796, in America's first contested national...</td>\n",
              "      <td>1960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>United States</td>\n",
              "      <td>1960-11-01</td>\n",
              "      <td>61620</td>\n",
              "      <td>Republican Party</td>\n",
              "      <td>61620_196011</td>\n",
              "      <td>61620_196011</td>\n",
              "      <td>PREAMBLE The United States is living in an age...</td>\n",
              "      <td>1960</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index    countryname       date  party         partyname          keys  \\\n",
              "0      0  United States 1960-11-01  61320  Democratic Party  61320_196011   \n",
              "1      1  United States 1960-11-01  61620  Republican Party  61620_196011   \n",
              "\n",
              "   manifesto_id                                               text  year  \n",
              "0  61320_196011  In 1796, in America's first contested national...  1960  \n",
              "1  61620_196011  PREAMBLE The United States is living in an age...  1960  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# add year column\n",
        "df_all = df_all.reset_index().assign(year = lambda column: column['date'].dt.year)\n",
        "df_all.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d307580-8bb0-4485-a7f9-ab3d6492a215",
      "metadata": {
        "id": "8d307580-8bb0-4485-a7f9-ab3d6492a215",
        "outputId": "99daa854-2973-4b89-b48b-46e8a23a9ae7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>ghg</th>\n",
              "      <th>greenhouse</th>\n",
              "      <th>net-zero</th>\n",
              "      <th>carbon</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">Japan</th>\n",
              "      <th>2014</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">United States</th>\n",
              "      <th>1960</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1964</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1968</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    ghg  greenhouse  net-zero  carbon\n",
              "Japan         2014    0           0         0       0\n",
              "              2017    0           0         0       0\n",
              "United States 1960    0           0         0       0\n",
              "              1964    0           0         0       0\n",
              "              1968    0           0         0       0"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_freq_by_country = {}\n",
        "for country_name, country_df in df_all.groupby(['countryname', 'year']):\n",
        "    vectorizer = CountVectorizer(\n",
        "        stop_words='english',\n",
        "        lowercase=True,\n",
        "        ngram_range=(1, 1),\n",
        "        vocabulary=vocab_list\n",
        "    )\n",
        "    counts = vectorizer.fit_transform(country_df['text']).toarray().sum(axis=0)\n",
        "    word_freq_by_country[country_name] = dict(zip(vectorizer.get_feature_names_out(), counts))\n",
        "df_timeseries = pd.DataFrame(word_freq_by_country).transpose()\n",
        "df_timeseries.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "175676a0-feed-42a4-9734-3234d5d4c791",
      "metadata": {
        "id": "175676a0-feed-42a4-9734-3234d5d4c791",
        "outputId": "64809362-4e08-4f09-cfed-261041e983d3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>level_0</th>\n",
              "      <th>level_1</th>\n",
              "      <th>level_2</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Japan</td>\n",
              "      <td>2014</td>\n",
              "      <td>ghg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Japan</td>\n",
              "      <td>2014</td>\n",
              "      <td>greenhouse</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Japan</td>\n",
              "      <td>2014</td>\n",
              "      <td>net-zero</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  level_0  level_1     level_2  count\n",
              "0   Japan     2014         ghg      0\n",
              "1   Japan     2014  greenhouse      0\n",
              "2   Japan     2014    net-zero      0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_long = df_timeseries.stack().to_frame('count').reset_index()\n",
        "df_long.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "029c27eb-ba25-4ded-9952-a1895004f932",
      "metadata": {
        "id": "029c27eb-ba25-4ded-9952-a1895004f932",
        "outputId": "2b9ecef7-66f1-4029-cca0-77b3245faea9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country</th>\n",
              "      <th>year</th>\n",
              "      <th>vocab</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Japan</td>\n",
              "      <td>2014</td>\n",
              "      <td>ghg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Japan</td>\n",
              "      <td>2014</td>\n",
              "      <td>greenhouse</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  country  year       vocab  count\n",
              "0   Japan  2014         ghg      0\n",
              "1   Japan  2014  greenhouse      0"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_long = df_long.rename({\"level_0\": 'country', 'level_1':'year', 'level_2': 'vocab'}, axis='columns')\n",
        "df_long.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7fde051-b880-4962-bcca-c135b49d590c",
      "metadata": {
        "id": "b7fde051-b880-4962-bcca-c135b49d590c"
      },
      "source": [
        "#### Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f140e7f9-2bed-4948-baf2-9bc623db814f",
      "metadata": {
        "id": "f140e7f9-2bed-4948-baf2-9bc623db814f"
      },
      "outputs": [],
      "source": [
        "df_long.to_csv(\"../data/processed/manifesto_us_japan_word_freq.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d63574c5",
      "metadata": {
        "id": "d63574c5"
      },
      "source": [
        "## Count with LLM with translation of Japanese\n",
        "It needs CUDA to run, running this with CPU might be slow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c94ba205",
      "metadata": {
        "id": "c94ba205"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from langdetect import detect\n",
        "from deep_translator import GoogleTranslator\n",
        "import re\n",
        "\n",
        "\n",
        "def translate_japanese_to_english(text, max_chunk_size=500):\n",
        "    # 文単位で分割（句点「。」「！」「？」などで）\n",
        "    sentences = re.split(r'(?<=[。！？])', text)\n",
        "\n",
        "    translated_sentences = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip()\n",
        "        if sentence:\n",
        "            try:\n",
        "                translated = GoogleTranslator(source='ja', target='en').translate(sentence)\n",
        "                translated_sentences.append(translated)\n",
        "            except Exception as e:\n",
        "                print(f\"翻訳エラー: {e}（文: {sentence}）\")\n",
        "\n",
        "    return ' '.join(translated_sentences)\n",
        "\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "def classify_climate_sentences(text):\n",
        "    # Load zero-shot classification pipeline\n",
        "    classifier = pipeline(\"zero-shot-classification\",\n",
        "                          model=\"facebook/bart-large-mnli\",\n",
        "                          device=0)\n",
        "\n",
        "    labels = [\"climate-related\"]\n",
        "\n",
        "    # Devide text into sentences\n",
        "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
        "\n",
        "    total_score = 0\n",
        "    count = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        result = classifier(sentence, candidate_labels=labels)\n",
        "        total_score += result[\"scores\"][0]\n",
        "        count += 1\n",
        "\n",
        "    # Calculate average score\n",
        "    average_score = total_score / count if count > 0 else 0\n",
        "    return average_score\n",
        "\n",
        "\n",
        "def multilang_classify_climate_sentences(text):\n",
        "    # Detect language\n",
        "    lang = detect(text)\n",
        "\n",
        "    # Translate to English if the text is in Japanese\n",
        "    if lang == 'ja':\n",
        "        txt = translate_japanese_to_english(text, max_chunk_size=500)\n",
        "    else:\n",
        "        txt = text\n",
        "\n",
        "    return txt, classify_climate_sentences(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "59650112-00d9-492b-a2e5-77e5bd7354c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        },
        "id": "59650112-00d9-492b-a2e5-77e5bd7354c4",
        "outputId": "b39fa5d0-ad9f-454f-d20e-912f0b44ebdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/41 [00:00<?, ?it/s]Device set to use cuda:0\n",
            "  5%|▍         | 2/41 [00:21<07:03, 10.86s/it]Device set to use cuda:0\n",
            "  7%|▋         | 3/41 [00:32<06:58, 11.00s/it]Device set to use cuda:0\n",
            " 10%|▉         | 4/41 [00:39<05:42,  9.26s/it]Device set to use cuda:0\n",
            " 12%|█▏        | 5/41 [00:47<05:16,  8.80s/it]Device set to use cuda:0\n",
            " 15%|█▍        | 6/41 [01:11<08:04, 13.85s/it]Device set to use cuda:0\n",
            " 17%|█▋        | 7/41 [01:21<07:15, 12.80s/it]Device set to use cuda:0\n",
            " 20%|█▉        | 8/41 [01:48<09:30, 17.29s/it]Device set to use cuda:0\n",
            " 22%|██▏       | 9/41 [02:13<10:26, 19.58s/it]Device set to use cuda:0\n",
            " 24%|██▍       | 10/41 [02:33<10:07, 19.61s/it]Device set to use cuda:0\n",
            " 27%|██▋       | 11/41 [02:53<09:55, 19.87s/it]Device set to use cuda:0\n",
            " 29%|██▉       | 12/41 [03:30<12:06, 25.06s/it]Device set to use cuda:0\n",
            " 32%|███▏      | 13/41 [04:03<12:51, 27.55s/it]Device set to use cuda:0\n",
            " 34%|███▍      | 14/41 [04:46<14:23, 31.97s/it]Device set to use cuda:0\n",
            " 37%|███▋      | 15/41 [05:16<13:41, 31.59s/it]Device set to use cuda:0\n",
            " 39%|███▉      | 16/41 [05:20<09:38, 23.14s/it]Device set to use cuda:0\n",
            " 41%|████▏     | 17/41 [05:59<11:14, 28.09s/it]Device set to use cuda:0\n",
            " 44%|████▍     | 18/41 [06:09<08:40, 22.63s/it]Device set to use cuda:0\n",
            " 46%|████▋     | 19/41 [06:43<09:28, 25.86s/it]Device set to use cuda:0\n",
            " 49%|████▉     | 20/41 [07:00<08:09, 23.31s/it]Device set to use cuda:0\n",
            " 51%|█████     | 21/41 [07:28<08:11, 24.57s/it]Device set to use cuda:0\n",
            " 54%|█████▎    | 22/41 [07:52<07:47, 24.62s/it]Device set to use cuda:0\n",
            " 56%|█████▌    | 23/41 [08:29<08:27, 28.17s/it]Device set to use cuda:0\n",
            " 59%|█████▊    | 24/41 [08:49<07:17, 25.75s/it]Device set to use cuda:0\n",
            " 61%|██████    | 25/41 [09:28<07:57, 29.87s/it]Device set to use cuda:0\n",
            " 63%|██████▎   | 26/41 [09:54<07:07, 28.51s/it]Device set to use cuda:0\n",
            " 66%|██████▌   | 27/41 [10:16<06:14, 26.73s/it]Device set to use cuda:0\n",
            " 66%|██████▌   | 27/41 [10:25<05:24, 23.17s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1788156747>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eng_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultilang_classify_climate_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m                 \u001b[0;31m# on the df using our wrapper (which provides bar updating)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m                     \u001b[0;31m# take a fast or slow code path; so stop when t.total==t.n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m                 \u001b[0;31m# Apply the provided function (in **kwargs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2895081492>\u001b[0m in \u001b[0;36mmultilang_classify_climate_sentences\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassify_climate_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-2895081492>\u001b[0m in \u001b[0;36mclassify_climate_sentences\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mtotal_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scores\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/zero_shot_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unable to understand extra arguments {args}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"This example is {}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1421\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChunkPipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1423\u001b[0;31m             return next(\n\u001b[0m\u001b[1;32m   1424\u001b[0m                 iter(\n\u001b[1;32m   1425\u001b[0m                     self.get_iterator(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_last\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1336\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/zero_shot_classification.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"use_cache\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_cache\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         model_outputs = {\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1854\u001b[0m             )\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1856\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1857\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1508\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m                     )\n\u001b[1;32m   1067\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m                     layer_outputs = encoder_layer(\n\u001b[0m\u001b[1;32m   1069\u001b[0m                         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                         \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \"\"\"\n\u001b[1;32m    548\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         hidden_states, attn_weights, _ = self.self_attn(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;31m# NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;31m# but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "tqdm.pandas()\n",
        "df_all['eng_text'], df_all['score'] = df_all['text'].progress_apply(multilang_classify_climate_sentences)\n",
        "\n",
        "df_all.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from langdetect import detect\n",
        "from deep_translator import GoogleTranslator\n",
        "from transformers import pipeline\n",
        "from datasets import Dataset\n",
        "from googletrans import Translator\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# GPUでモデルをロード（1回だけ）\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)\n",
        "labels = [\"climate-related\"]\n",
        "\n",
        "# 翻訳関数（文単位で分割して翻訳）\n",
        "\n",
        "def translate_japanese_to_english(text):\n",
        "    translator = Translator()\n",
        "    sentences = re.split(r'(?<=[。！？])', text)\n",
        "    translated_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "       sentence = sentence.strip()\n",
        "       if sentence:\n",
        "          try:\n",
        "              translated = translator.translate(sentence, src='ja', dest='en')\n",
        "              if translated and hasattr(translated, 'text') and translated.text:\n",
        "                    translated_sentences.append(translated.text)\n",
        "              else:\n",
        "                  print(f\"翻訳失敗: {sentence}\")\n",
        "          except Exception as e:\n",
        "              print(f\"翻訳エラー: {e}（文: {sentence}）\")\n",
        "    return ' '.join(translated_sentences)\n",
        "\n",
        "# 多言語対応分類関数（翻訳＋分類）\n",
        "def multilang_classify_climate_sentences_batch(batch):\n",
        "    texts = batch[\"text\"]\n",
        "    eng_texts = []\n",
        "    valid_indices = []\n",
        "\n",
        "    # 翻訳とフィルタリング\n",
        "    for i, text in enumerate(texts):\n",
        "        lang = detect(text)\n",
        "        if lang == 'ja':\n",
        "            translated = translate_japanese_to_english(text)\n",
        "            if translated.strip():\n",
        "                eng_texts.append(translated)\n",
        "                valid_indices.append(i)\n",
        "            else:\n",
        "                eng_texts.append(\"\")  # 空でも追加\n",
        "        else:\n",
        "            if text.strip():\n",
        "                eng_texts.append(text)\n",
        "                valid_indices.append(i)\n",
        "            else:\n",
        "                eng_texts.append(\"\")  # 空でも追加\n",
        "\n",
        "    # 分類（空文字列は除外）\n",
        "    scores = [0.0] * len(texts)\n",
        "    if any(t.strip() for t in eng_texts):\n",
        "        non_empty_texts = [t if t.strip() else \"empty\" for t in eng_texts]\n",
        "        results = classifier(non_empty_texts, candidate_labels=labels)\n",
        "        for i, r in enumerate(results):\n",
        "            scores[i] = r[\"scores\"][0]\n",
        "\n",
        "    return {\"eng_text\": eng_texts, \"score\": scores}\n",
        "\n",
        "\n",
        "# DataFrameの例（df_all に 'text' カラムがある前提）\n",
        "# df_all = pd.DataFrame({'text': [...日本語や英語の文章...]})\n",
        "\n",
        "# DataFrame → Hugging Face Dataset に変換\n",
        "dataset = Dataset.from_pandas(df_all)\n",
        "\n",
        "# バッチ処理で翻訳＋分類（GPU活用）\n",
        "dataset = dataset.map(multilang_classify_climate_sentences_batch, batched=True, batch_size=16)\n",
        "\n",
        "# Dataset → DataFrame に戻す\n",
        "df_all_result = dataset.to_pandas()\n",
        "\n",
        "# 結果を元の DataFrame に統合\n",
        "df_all[\"eng_text\"] = df_all_result[\"eng_text\"]\n",
        "df_all[\"score\"] = df_all_result[\"score\"]\n",
        "\n",
        "# 結果表示\n",
        "df_all.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425,
          "referenced_widgets": [
            "9835f0dbade84163b3c9010ac5968a54",
            "8f36af6c0d36459d915c174ade6017e7",
            "01b431dce1ee4c51a1eaa9e35aafcf4a",
            "7851bbab49b74f7eb1459874e5443bd1",
            "3f214e22414c4f3a967f856238f8f430",
            "298f05748bb84512b70a7db418ef3748",
            "51c5a0e5396b41e0a8d67be0c48117e3",
            "dbe1ff43fc514ce9b58dec722e24c3cc",
            "cc06cd84ceda465598df0aab3691e676",
            "6610a61cc3a84265a85f160804dd47e1",
            "aa370faa9cae4e5cbf9ecef713c51843"
          ]
        },
        "id": "dnwaF_TEW451",
        "outputId": "7bb4cc51-fdab-44f5-ad05-77b391d6a756"
      },
      "id": "dnwaF_TEW451",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/41 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9835f0dbade84163b3c9010ac5968a54"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "翻訳エラー: the JSON object must be str, bytes or bytearray, not NoneType（文: 株式会社の参入促進をはじめとする医療法人、社会福祉法人の制度改革。）\n",
            "翻訳エラー: The read operation timed out（文: 原発の稼働がなくとも 日本経済は成り立ちます。）\n",
            "翻訳エラー: the JSON object must be str, bytes or bytearray, not NoneType（文: 現行憲法は、日本の民主主義を進展させ、戦後秩序の基本となりました。）\n",
            "翻訳エラー: the JSON object must be str, bytes or bytearray, not NoneType（文: 北朝鮮の脅威から、 国民を守り抜きます わが国の上空を飛び越える弾道ミサイルの相次ぐ発射、核実験の強行など、 北朝鮮による挑発行為はエスカレートし、重大かつ差し迫った脅威となっています このような時こそ、世界をリードできる、経験豊かで安定した政権が必要です わが党は平和に向けた外交努力を続け、断固、国民を守り抜きます 北朝鮮に対する国際社会による圧力強化を主導し、完全で検証可能かつ不可逆的な方法で すべての核・弾道ミサイル計画を放棄させることを目指すとともに、拉致問題の解決に全力を尽くします 日米同盟をより一層強固にすることで、わが国の抑止力を高めます ミサイル対処能力の強化をはじめ、国民保護を最優先に対応し、国民の生命と財産を守り抜きます 世界の中心で、動かす外交 アベノミクスの加速で、景気回復・デフレ脱 却を実現します 全力を傾注したアベノミクスの5年間 いま、多くの指標が示す通り、わが国の経済は確実に回復しています この流れを確かなものにするため、「生産性革命」と「人づくり革命」の2つの大改革を断行することによって、 力強い消費を実現し、経済の好循環を完遂します アベノミクス5年間の実績名目GDP 過去最高 50兆円増加493 兆円(2012 年 10-12 月期)➡ 543 兆円(2017 年 4-6 月期) 就業者数 185万人増加6,271 万人(2012 年)➡ 6,456 万人(2016 年)  正社員有効求人倍率 初の1倍超え 0.5 倍(2012 年 2 月)➡ 1.01 倍(2017 年 7 月) 若者の就職内定率 過去最高 大学生 93.9%(2013 年 4 月)➡ 97.6%(2017 年 4 月) 企業収益 過去最高 26.5兆円増加 48.5 兆円(2012 年度)➡ 75.0 兆円(2016 年度) 家計の可処分所得 2年連続で増加 292 兆円(2012 年)➡ 295 兆円(2015 年)  外国人旅行者数 5年で約3倍 870 万 8 千人(2012 年度)➡ 2,482 万 4 千人(2016 年度) 未来に責任を持つ確かな政策で、 さら な る ス テ ー ジ へ 2020年までの3年間を生産性革命の「集中投資期間」として、 中小企業・小規模事業者も含め、大胆な税制、予算、規制改革などあらゆる施策を総動員して、 企業の収益を設備投資や人材投資へ振り向けていきます 「起業大国」を目指し、産業の新陳代謝と世界を変える「ユニコーン・ベンチャー」創出に向け、中長期の成長資金の供給拡大などの取り組みを加速します 中小企業・小規模事業者の円滑な世代交代・事業承継に資するよう、 税制を含めた徹底した支援を講じます 働 く皆さんの様々な声に耳を傾け、多様な形で働き、参加できる社会をつくります このため、「働き方改革」を実行します 劇的な生産性の向上で、国民の所得を増やします 暮らしの安心を守り抜く 少子高齢化社会の到来が急速に進んでいる現在、輝く「人生 100 年時代」を迎えるためには、 国 民 の 多 く が 不 安 に 感 じ て い る「 子 育 て ・ 介 護 」の 問 題 を 解 決 す る こ と が 不 可 欠 で す こ の た め 、「 人 づ く り 革 命 」 を 断 行 し ま す 政 策 資 源 を 大 胆 か つ 集 中 的 に 投 入 す る こ と で 、 お 年 寄 り も 若 者 も 安 心 し て 暮 ら し 、活 躍 で き る「 全 世 代 型 社 会 保 障 」を 目 指 し ま す 暮らしの安心を守り抜く 幼児教育無償化を一気に加速します 2020年度までに、3歳から5歳までのすべての子供たちの幼稚園・ 保育園の費用を無償化します 0歳から2歳児についても、所得の低い世帯に対して無償化します 待機児童解消を達成するため、「子育て安心プラン」を前倒しし、 2020 年度までに、32 万人分の保育の受け皿整備を進めます 真に支援が必要な所得の低い家庭の子供たちに限って、高等教育の無償化を図ります このため、必要な生活費をまかなう給付型奨学金や授業料減免措置を大幅に増やします 介護人材の確保に向けて、介護職員のさらなる処遇改善を進めます これらの施策を実行するために、消費税10%時の増収分について、社会保障の充実と財政健全化とのバランスを取りつつ、子育て世代への投資を集中することで、 「全世代型社会保障」へと大きく舵を切ります 本年末までに、「人づくり革命」に関する2 兆円規模の新たな政策パッケージを取りまとめます 同時に、財政健全化の旗は明確に掲げつつ、不断の歳入・歳出改革努力を徹底します 地域の暮らしを守り抜く 地方創生で活力ある元気な地方をつくります 地方の元気なくして日本の再生はありません 地域未来投資をはじめとする、地方が自主的に取り組む政策を応援し、 地方が主役の「地方創生」を実現します そのために、必要な対策を総合的に実施します 中小企業・小規模事業者の生産性向上に向けて、きめ細かなあらゆる政策を総動員して支援します 若者や意欲のある農林漁業者が夢や希望を持てる「農政新時代」を切り拓きます 外国人旅行者4,000万人を目指し、地域の特色を活かした観光資源を磨き上げるとともに、受け入れ体制を強化します 地方大学の魅力向上に取り組み、若者の地方での就学・就業を促進します 政府関係機関の地方への移転に取り組み、企業の本社機能の地方移転も積極的に支援します さらに復興加速へ 災害から国民の生命と財産を守るのが、政治の責任です 東日本大震災、熊本地震、九州北部豪雨災害等からの復興を加速するとともに、 自然災害から国民生活を守るため、防災・減災に戦略的に取り組む国土強靭化を推進し、 災害に強い街づくりを進めます 東日本大震災による地震・津波被災地域の復興については、復興期間が終了する 2020 年度までに必ずやり遂げる、という強い意志をもって全力で取り組んでいきます 原子力災害からの復興を目指す福島については、復興期間後も国が前面に立って中長期的、 計画的な見通しのもとに支援を継続し、避難しておられる方々が安心して帰還できるよう取り組みます 熊本地震からの復興については、道路、鉄道、港湾等の基幹インフラの整備や 被災地の住宅再建・宅地の復旧等の支援を着実に推進します 頻発する自然災害からの一日も早い復旧・復興に努めます この国の未来を切り拓く 国民の幅広い理解を得て、 憲法改正を目指します 現行憲法の「国民主権」、「基本的人権の尊重」、「平和主義」の 3つの基本原理は堅持しつつ、憲法改正を目指します 憲法改正については、国民の幅広い理解を得つつ、衆議院・参議院の憲法審査会で 議論を深め各党とも連携し、自衛隊の明記、教育の無償化・充実強化、 緊急事態対応、参議院の合区解消など 4 項目を中心に、党内外の十分な議論を踏まえ、憲法改正原案を国会で提案・発議し、国民投票を行い、初めての憲法改正を目指します 自民党 政策 BANK 未来に責任を持つ確かな政策で、 さらなるステージへ I 経済再生 生産性革命 「 少 子 高 齢 化 」 と い う 最 大 の 壁 に 立 ち 向 か う た め 、「 生 産 性 革 命 」と「 人 づ く り 革 命 」を 断 行 し ま す こ の 2 つ の 大 改 革 の 新しい経済政策パッケージを年内に取りまとめます ロボット・IoT・人工知能(AI)といった生産性を劇的に押し 上 げ る 最 先 端 の イ ノ ベ ー シ ョ ン を 起 こ し 、「 生 産 性 革 命 」 を 実現します 2020年までの3年間を「生産性革命・集中投資期間」とし て、大胆な税制、予算、規制改革などあらゆる施策を総動員 します 人手不足や高齢化を補うために、ロボット・IoT・人工知能 (AI)など、企業の生産性を飛躍的に高める投資を推進しま す 特に生産性の低い業種や、中堅企業・中小企業・小規模 事業者に対しては集中的に支援します 「生産性革命」の果実を賃上げに充てる政策支援を拡充し ま す こ れ に よ り 所 得 を 大 き く 増 や し て 、デ フ レ 脱 却 へ の ス ピードを最大限加速します 自動走行、健康・医療・介護、生産性の向上を始めとする、 第4次産業革命のイノベーションの社会実装を加速しま す 革新的なビジネスモデルと新たな産業群を創出すると ともに、社会課題の解決にも繋げます コーポレートガバナンスの強化などにより、投資拡大、研究 開発、人材育成などの経営資源の最適な活用を経営者に 促します 「起業大国」を目指し、産学官及び金融機関の連携による中長 期の成長資金の供給拡大を含め、産業の新陳代謝と「ユニ コーン・ベンチャー」創出に向けた取り組みを加速します 中小企業・小規模事業者の円滑な世代交代・事業承継に資 するよう、税制を含め徹底した支援を講じます わが国の潜在的成長力を高め、新たな需要を掘り起こして い く た め 、戦 略 的 な イ ン フ ラ マ ネ ジ メ ン ト の 推 進 や 、建 設 、 物流分野等の生産性向上、自動運転等の新技術の社会実 装を進めること等により、「生産性革命」を推進します 建 設 現 場 に お い て 、調 査 ・ 測 量 、設 計 、施 工 及 び 維 持 管 理 ・ 更新のあらゆるプロセスでICTの活用を進める等により、建 設現場の生産性を大幅に向上させます わが国の経済成長に向けた「生産性革命」の更なる加速や 度重なる災害から国民の生命と財産を守る国土強靭化の ため、必要な対策を総合的に実施します AI・IoTをはじめとした技術革新を生産性向上と社会課題 解 決 に つ な げ る た め 、「 官 民 デ ー タ 活 用 推 進 基 本 法 」 に 基 づ き 、自 動 運 転 や ス マ ー ト 農 業 等 の 身 近 な 場 面 で 、ビ ッ グ データを使える環境を整備していきます マイナンバー制度の利活用を推進し、各種手続がオンライ ンで完結する国民視点の行政システムへの転換を図るとと もに、利用範囲の拡大を進め、安心安全にオンラインサー ビスを利用する際のカギとなるマイナンバーカードの普及 促進を図ります 人づくり革命 子育て世代への投資、社会保障の充実、財政健全化にバ ランスよく取り組みつつ、「人づくり革命」を力強く進めてい くため、消費税率10%への引き上げに伴う増収分などを活 用した2兆円規模の新たな政策を本年末までにとりまとめ ます 幼児教育の無償化や介護人材の確保などを通じてわが国 の社会保障制度を全世代型社会保障へ大きく転換すると ともに、所得の低い家庭の子供に限った高等教育無償化や リ カ レ ン ト 教 育 の 充 実 な ど 人 へ の 投 資 を 拡 充 し 、「 人 づ く り 革命」を力強く推進します 意欲と能力のある子供たちが経済的理由により専修学校 や 大 学 へ の 進 学 を 諦 め る こ と の な い よ う 、授 業 料 の 減 免 措 置の拡充や給付型奨学金の支給額を大幅に増やすことで、 真に支援が必要な所得の低い家庭の子供に限って高等教 育の無償化を実現します 併せて徹底的な大学改革に取り 組みます 働き方改革 働き方改革を推進することで、長時間労働を是正するとと もに、賃金などの待遇について、雇用形態ではなく、職務内 容によって公正に評価される仕組みを導入します 長時間労働の是正や「同一労働同一賃金」の実現など多様 なライフスタイルを実現する働き方改革を推進するととも に、最低賃金1,000円を目指します 各産業や地域の中小 企業の実情を踏まえたきめ細かい支援を行います 女性・若者・高齢者、 障害や病気のある方やその家族など 誰 も が 意 欲 と 能 力 に 応 じ て 就 労 や 社 会 参 加 で き る よ う 、ガ イドラインの制定など実効性のある政策手段を講じてテレ ワークや副業・兼業などの柔軟で多様な働き方を進めると ともに、就労支援、生活支援、居場所作りを進めます 地域の経済・雇用を支える建設業や自動車運送事業(ト ラ ッ ク ・ バ ス ・ タ ク シ ー )、自 動 車 整 備 事 業 に つ い て 、長 時 間 労働の是正等の労働環境の改善を図り、働き方改革を推 進します 女性活躍 政治の場への女性の更なる参画を促進するため、「政治分 野における男女共同参画推進法」の早期成立を目指しま す わが党としては、中央政治大学院などを活用し、女性候 補者の育成を率先垂範します 指導的地位に占める女性の割合を3割程度にすることを 目指し、女性参画の拡大や将来に向けた人材育成を進めま す 「 女 性 活 躍 推 進 法 」 に 基 づ き 、 企 業 等 に よ る 行 動 計 画 の 策定と情報の見える化を徹底するとともに、「女性の活躍推 進 企 業 デ ー タ ベ ー ス 」を 改 善 ・ 充 実 し て 、労 働 市 場 ・ 資 本 市 場での活用を促します 女性の新しいキャリア・ステージの形である起業を支援し ます 女性起業家向けの情報発信、資金調達への支援、 ロールモデルの充実、両立支援のための取組など、女性の 起業ステージに応じた伴走型の支援を実現します これまで女性の活躍が少なかった自動車関連や建設業分 野において、女性も働きやすい職場環境の整備や業務の 魅力発信等を行い、就業者数の大幅増を目指します 女性に対するあらゆる暴力を根絶します 性犯罪・性暴力 被害者支援のワンストップ支援センターを全都道府県に整 備 ・ 拡 充 し ま す 婦 人 保 護 事 業 の 法 的 な 措 置 を 含 め た 抜 本 的な見直しを行います DVやストーカーの被害者の支援や加害者に対する取組を 進めます いわゆるアダルトビデオ出演強要問題・「JKビジ ネス」問題等の被害を根絶するための対策を推進します 「女性の健康の包括的支援に関する法律」の成立を目指し ます ひとり親家庭に対し、仕事と子育ての両立支援、孤立化させ ないための居場所の確保などの支援を拡充します 家事や子育ては女性が担うべきとする古い意識や風土を 改め、「イクメン」や、妊娠・出産した本人やその配偶者の働 き方を適切に管理する「イクボス」も含め、男性の意識改革 と職場風土の改革を進めます 男性の育児休暇の取得及 び家事・育児への参画の促進に取り組みます 旧姓の幅広い使用を認める取組を進めます まずは、住民 基本台帳とそれに連動するマイナンバーカードにおいて旧 姓併記ができるよう準備を進めます また、パスポートへの 旧姓併記の拡大に向けた検討や、銀行口座についても旧 姓使用が可能となるよう働きかけを行うなど、取組を進め ていきます 経済再生 「 6 0 0 兆 円 経 済 の 実 現 」 、「 希 望 出 生 率 1 . 8 」 、「 介 護 離 職 ゼ ロ 」と い う「 新 ・ 三 本 の 矢 」を 引 き 続 き 一 体 的 に 推 進 し ま す 経済成長の成果を、子育て・介護等に分配して、それを更な る成長につなげる「成長と分配の好循環」を創り上げます 未だアベノミクスの恩恵を十分に実感できていない地方の 隅 々 ま で 暖 か い 風 を 届 け る べ く 、ロ ー カ ル ・ ア ベ ノ ミ ク ス を 力強く推進します 成長の果実を、大企業から中小企業・小規模事業者まで行 き渡らせるため、適正なコスト転嫁を図るなどの環境整備 を進めます イノベーションによる生産性の向上と働き方改革による潜在 成長率の引き上げ、国民の新たな需要の掘り起こし、海外需 要の取り込みを通じ、デフレ脱却を確実なものとします 経済成長や企業の収益に見合った実質賃金の上昇、最低 賃 金 の 引 上 げ を 図 り 、こ の 流 れ を 中 小 企 業 ・ 小 規 模 事 業 者 や非正規雇用へも広げ、消費の拡大に結び付けます 経済成長が財政健全化を促し、財政健全化の進展が経済 の一段の成長に寄与するという好循環を加速し、財政再建 と経済成長の両立を図ります 第4次産業革命のイノベーションをあらゆる産業や生活に 取り入れて「Society 5.0」を実現すべく、成長戦略(「未来 投資戦略2017」)を確実に実行します イノベーションが可能にする魅力的なビジネスを世界に先 駆けて実現させるため、岩盤規制改革に徹底的に取り組み ます 岩盤規制改革で、これまで大きな成果をあげてきた国家戦 略特区は、透明性を向上し国民に分かりやすい運用を行い つ つ 、残 さ れ た 岩 盤 を 打 破 し ま す ま た 、特 区 で 実 現 し た 規 制改革について、できるだけ早期に全国展開を図ります 規 制 の「 サ ン ド ボ ッ ク ス 」制 度 創 設 に 向 け 、次 期 通 常 国 会 に 関連法案を提出します 事前規制を最小限にする代わりに 事 後 チ ェ ッ ク 体 制 を 整 え 、自 動 走 行 、ド ロ ー ン 、F i n T e c h な ど新たな社会の実現に向けたさまざまな分野でのチャレン ジを後押しします コーポレートガバナンス改革を「形式」から「実質の充実」へ と深化させ、中長期的な企業価値向上と投資家へのリター ンの拡大という好循環を実現させます NISA・ジュニアNISA・つみたてNISAの普及などを通じ て 、長 期 分 散 投 資 の 推 進 等 ポ ー ト フ ォ リ オ ・ リ バ ラ ン ス を 促 す環境整備などを積極的に進め、国民の安定的な資産形 成を支援します 東京国際金融センター化の推進など、国際金融センターと しての機能を強化するとともに、総合取引所の実現を含め、 金融・証券市場の活性化・利便性向上を図ります ● 地域産業や企業の成長と生産性の向上などを図るため、地域金融の金融仲介機能を強化し、企業の事業性の評価に 基づく融資や経営支援など、地域経済の活性化に向けた取 り組みを促進しま す ● FinTechにおける国際標準の主導と、利用者目線での金融 サービスの革新との観点に立ち、FinTechエコシステムの構 築 な ど 、オ ー プ ン ・ イ ノ ベ ー シ ョ ン を 推 進 す る た め の 環 境 整備を進め、日本発グローバルFinTechへの戦略的取り組み を強化します 起業者が活動しやすいビジネス環境を整備するため、会社 の設立登記手続の迅速化を図ります 次世代の基幹産業と目されるわが国の優れたインフラシス テムの海外展開について、官民ファンド等を活用し、民間企 業の海外展開を支援するとともに、政府のトップセールス 等を戦略的に駆使し、受注競争を勝ち抜きます 地域経済におけるバリューチェーンの中心的な担い手であ る「地域未来牽引企業」とその取引群に対し、あらゆる支援 を 重 点 投 入 す る ( 全 国 2 0 0 0 社 程 度 ) な ど 、「 地 域 へ の 未 来 投 資 」を 拡 大 し 、今 後 3 年 程 度 で 投 資 拡 大 1 兆 円 、G D P 5 兆 円の押上げを目指します 「IR(統合型リゾート)推進法」に基づき、様々な懸念に万全 の対策を講じて、大人も子供も楽しめる安心で魅力的な 「日本型IR」を創り上げます 子育て世代に対する安価な住宅の供給や三世代同居・近 居を推進します 大幅に拡充した住宅ローン減税と減税の効果が限定的な 所 得 層 に 対 す る「 す ま い 給 付 金 」の 給 付 措 置 、住 宅 取 得 等 資金に係る贈与税の非課税措置を引き続き講じるなど、住 宅投資の活性化を図ります 不動産市場を支える制度面の整備により、不動産市場の活 性 化 を 図 る と と も に 、リ フ ォ ー ム 産 業 の 活 性 化 と あ わ せ 、適 正な建物評価の定着、取引市場環境の整備等を通じ、既存 住宅市場の活性化を図ります 空き家の除却を推進するとともに、空き家バンクの構築等 による流通の促進、用途変更等を円滑にする建築規制の合 理化等により、空き家の活用を推進するとともに、低未利用 地の利用促進を図ります 所有者不明土地について、地籍調査を推進するとともに、 公的機関の関与により、地域ニーズに対応した幅広い公共 的目的のための活用を可能とする新たな仕組みを構築し ます 所有者を特定することが困難な土地に関して、地域の実情 に 応 じ た 適 切 な 利 用 や 管 理 が 図 ら れ る よ う 、長 期 間 相 続 登 記が未了の土地の解消方法の検討を進めるとともに、今後 の人口減少に伴って所有者を特定することが困難な土地 が増大しないよう、登記制度や土地所有権のあり方等の中 長期的課題に本格的に取り組みます 登記所備付地図の整備等により不動産情報基盤の充実を 図ります 法定相続情報証明制度の利用範囲を拡大し、所有者情報 の収集・整備・利活用を推進するため、制度・体制の両面か ら更なる取組を進めるとともに、公的機関における遺言書 の保管制度の創設に取り組み、相続に係る紛争を防ぐとと もに、相続登記を促進します 整備新幹線の新函館北斗―札幌間、金沢―敦賀間、武雄温 泉―長崎間は、政府・与党申合せ等に基づき、開業効果を で き る 限 り 早 期 に 発 揮 で き る よ う 取 り 組 み ま す ま た 、与 党 でルートを決定した敦賀-大阪間について財源を確保し つ つ 早 期 着 工 を 目 指 す と と も に 、リ ニ ア 中 央 新 幹 線 の 東 京 ― 大 阪 間 の 早 期 全 線 開 通 を 目 指 し ま す さ ら に 、新 幹 線 の 基本計画路線をはじめとして、地方創生に役立つ幹線鉄道 ネットワークの構築に向けて取り組みます 航空自由化(オープンスカイ)の戦略的な推進や諸外国と のイコールフッティングを踏まえた空港使用に係るコスト の見直し等を通じ、国際競争力の強化を図ります テロ対策を含む空港保安体制の強化やビジネスジェット利 用 環 境 の 改 善 、地 方 送 客 の た め の 国 内 地 方 ネ ッ ト ワ ー ク 利 活用の充実、計画的なLCC参入促進、空港アクセスの充実 等を通じ、空港機能の整備強化を図ります 航空機の操縦士・乗務員・整備士・航空管制官等の航空従 事者の養成・確保を推進します 地域の産業物流や農林水産物の輸出を支える港湾・航路 整 備 の 推 進 を 図 る と と も に 、国 際 バ ル ク 戦 略 港 湾 の 整 備 や 国際コンテナ戦略港湾の整備、港湾の耐震化による災害対 応機能の強化を図ります 地域経済を支える基盤として欠かせない道路ネットワーク の整備について、民間施設に直結するインターチェンジも 含 め た 着 実 な 推 進 を 図 る と と も に 、ス マ ー ト I C や「 道 の 駅 」 について総合的な支援を行います 民間投資の喚起による成長力強化の実現のため、官民の連 携 に よ り 社 会 資 本 の 整 備・運 営・更 新 を 行 う た め の 基 本 法 を制定します 空港、水道、下水道、道路のコンセッション事 業等、PPP/PFIの積極的な推進を図り、地域の活性化を 進めます 高速道路料金については利用重視の観点から、実施目的 が明確で効果の高い割引を行うとともに、適切な維持管 理 ・ 更 新 へ 対 応 し た も の に し ま す 大 都 市 圏 と 地 方 の 道 路 利用の状況を鑑み、わかりやすい料金に整理するとともに、 大 都 市 圏 に つ い て は 、環 状 道 路 時 代 に ふ さ わ し く 、交 通 流 を最適化する料金施策の導入に取り組みます 世界最速・最高品質の審査体制の実現、地方と中小・ベン チャー企業の知財活用促進、知財など無形資産の適切な 評価、第4次産業革命を加速する著作権制度の早期実現 など知財システムの整備、知財創造教育の充実等の知的 財 産 ・ 標 準 化 戦 略 を 成 長 の 基 盤 と し て 推 進 し 、世 界 最 高 の 知財立国を目指します 「衣」「食」「住」やコンテンツ(アニメ、ドラマ、音楽、映画など) をはじめ「日本の魅力」の海外発信・展開や海外来訪者の受 入を進めるクールジャパン政策を成長戦略の一翼と位置づ け、支援策、人材の育成・人材ハブの構築、国内外のクール ジャパン拠点構築等の振興策を積極的に展開します 日本産酒類の海外展開を推進するため、情報発信の強化 や 、ブ ラ ン ド 力 の 向 上 、輸 出 環 境 の 整 備 な ど に よ る 国 際 競 争力の強化などの対策を強力に推進します 2025年大阪・関西万博の誘致を成功させるため、国を挙 げ て 取 り 組 み ま す 来 年 の 開 催 国 決 定 投 票 に 向 け 、各 国 へ の働きかけを強力に進めます 財政再建 基礎的財政収支を黒字化するとの目標は堅持します 同時 に、債務残高対GDP比の安定的な引き下げも目指します 引き続き歳出・歳入両面からの改革を進め、目標達成に 向けた具体的計画を策定します 2019年10月に消費税率を10%へ引き上げます その際、 「 全 世 代 型 社 会 保 障 」へ の 転 換 な ど「 人 づ く り 革 命 」を 実 現 するため、消費税率10%への引上げの財源の一部を活用 します 子育て世代への投資と社会保障の安定化とにバラ ンスよく充当し、景気への悪影響を軽減しながら財政再建 も確実に実行します 2019年10月の軽減税率制度の導入に当たっては、基礎 的 財 政 収 支 を 黒 字 化 す る と の 目 標 を 堅 持 す る 中 で 、「 社 会 保障と税の一体改革」の原点に立って安定的な恒久財源を 確 保 し ま す 併 せ て 、混 乱 な く 円 滑 に 導 入 で き る よ う 、事 業 者への対応を含め、万全の準備を進めていきます 経済社会の構造変化を踏まえた個人所得課税改革を行います 所得再分配機能の回復や多様な働き方に対応した仕 組みなどを目指す観点から、各種控除の見直しなどの諸課 題に取り組んでいきます 科学技術 「 科 学 技 術 力 は 国 力 に 直 結 す る 」と の 考 え の も と 、「 世 界 で 一 番 イ ノ ベ ー シ ョ ン に 適 し た 国 」 を 目 指 し 、「 第 5 期 科 学 技 術基本計画」に基づき、「Society5.0」の実現に向けた科学 技術イノベーションの活性化を官民挙げて推進するととも に、5年間総額26兆円の政府研究開発投資を目指します 世界最高水準の研究拠点の形成や人材の育成・確保を行 い ま す ま た 、人 工 知 能 、材 料 、光 ・ 量 子 な ど の 先 端 的 な 研  究開発を支援し、産学官共創システムを構築します i P S 細 胞 な ど の 健 康 ・ 医 療 や 、防 災 ・ 減 災 、省 エ ネ 及 び 核 融合、H3ロケットなどの宇宙航空、海洋・極域の各分野、も んじゅの廃炉を含めた安全確保対策や、原子力分野の研究 開発を推進します G空間社会の実現に向け、共通情報基盤であるG空間情報 センターの利活用を促進し、来年度の準天頂衛星4機体制  による運用を見据え、防災・農業・交通等の様々な分野での 新産業・新サービスの具体化を推進します 新たな産業フロンティアとして国際争奪競争になっている 宇宙分野において、高精度測位を可能とする準天頂衛星シ ステムや衛星データのオープン&フリー化やデータセン ターの整備、ベンチャー支援などにより、宇宙産業の倍増を 目指します エネルギー 「 エ ネ ル ギ ー 基 本 計 画 」を 踏 ま え 、徹 底 し た 省 エ ネ 、再 生 可 能エネルギーの最大限の導入、火力発電の高効率化など に よ り 、原 発 依 存 度 を 可 能 な 限 り 低 減 さ せ ま す ま た 、安 定 供給を確保し、経済成長とCO2排出抑制を両立させるバラ ンスの取れたエネルギーミックスの実現に向け、責任ある エネルギー政策を遂行します 再生可能エネルギーの最大限の導入と国民負担の抑制の 両立を図るため、技術開発、規制改革、系統制約の克服 などを進めるとともに、徹底的な省エネ対策、ゼロエミッ ション技術の導入を行い、カーボンフリー社会を目指しま す また、これらの分野における新産業創出や地方創生を 通じて投資を拡大し、GDP600兆円への起爆剤とします 原子力は安全性の確保を大前提に、エネルギー需給構造 の安定性に寄与する重要なベースロード電源との位置付 け の も と に 活 用 し ま す い か な る 事 情 よ り も 安 全 性 を 最 優 先し、原子力規制委員会によって世界最高レベルの新規制 基準に適合すると認められた場合には、立地自治体等関係 者の理解と協力を得つつ、原発の再稼働を進めます エネルギーシステム改革の成果を活かした電気料金の抑 制などによる経済基盤の強化や、分散型エネルギーシステ ムの導入によるエネルギーの地産地消を進めて、地域経済 の活性化を図ります 水素社会実現のため、基本戦略を策定し、燃料電池(燃料 電池自動車、家庭用燃料電池など)の導入、水素ステーショ ンの戦略的な整備、水素発電、国際的な水素サプライ チェーンの構築に向けた技術開発や各省庁にまたがる規 制の改革を進め、将来のエネルギーの新たな選択肢を創 出します わが国企業が安定的に資源開発投資を行えるよう、リスク マネー供給を強化して、国外の権益確保を支援し、供給源 の 多 角 化 を 図 り ま す メ タ ン ハ イ ド レ ー ト ・ レ ア ア ー ス 泥 等 の海洋資源戦略の推進を加速します また柔軟なLNG市 場形成をリードし、調達コスト削減を目指します 観光立国 外国人旅行者2020年4,000万人・旅行消費額8兆円を目 指し、訪日プロモーションの強化やビザ緩和、免税店の拡大 電子化等利便性の向上、空港・港湾のCIQ強化等、多様な ニーズに応じた受け入れ体制の整備・強化を図ります 観光庁や日本政府観光局の組織体制の拡充、受益者負担 の考えに基づき、高次元で観光施策を実行するために必要 となる追加的な観光財源の確保に取り組み、観光立国実現 に向けた観光基盤の拡充・強化を図ります 違法民泊業者・提供者の厳格な排除・取締りを行うととも に、 ルールに則った民泊については、ホテル・旅館業者への 適切な支援、宿泊施設の需給バランスや宿泊を通じた日本 文化理解に対する多様なニーズを鑑みつつ、規制緩和や ルール整備に積極的に取り組みます 広域観光の推進や、休暇・休祝祭日の機能的な活用 、国際 クルーズ拠点の形成、ジャパンレールパス等の利便性向上 やICT利活用による観光地・宿泊施設の多言語対応・情報 発信の強化を図り、観光産業の活性化を図ります 文化庁・観光庁等省庁間連携を強化するとともに、公的施 設( 迎 賓 館 等 )や 伝 統 文 化 財 等 の 観 光 資 源 と し て の 戦 略 的 利 活 用 景 観 ・ 街 並 み の 整 備 、医 療 ・ ア ー ト ・ 産 業 と い っ た ニ ュ ー ツ ー リ ズ ム の 振 興 に 取 り 組 み 、国 内 観 光 資 源 の 強 化 を図ります わが国の恵み豊かな森里川海を守り、人と自然が共に生き る地域づくりを進め、国立公園や世界自然遺産の適切な保 全 、安 全 で 快 適 な 利 用 環 境 の 整 備 に よ り 、観 光 の 振 興 を 図 ります 2020年 東京オリンピック・パラリンピック競技大会及び地 方の観光地における外国人への対応に向けて 、「言葉の 壁」をなくす多言語音声翻訳の普及を図ります 2020年 東京オリンピック・パラリンピック 「復興五輪」として被災地が復興を成し遂げつつある姿を 世界に発信するとともに、競技開催地だけのイベントとする ことなく、日本全国の祭典となるよう、参加国・地域との交流 を全国的に展開します パラリンピックのレガシー(遺産)として、心のバリアフリー の 推 進 や 公 共 交 通 機 関 、建 築 物 、道 路 等 の バ リ ア フ リ ー 化 を進め、障害者も高齢者も健常者も共生できるユニバーサ ルデザインの社会をつくります 大会開催時の交通混雑緩和に取り組み、大会の成功と経 済・市民活動との両立を目指すとともに、スポーツ産業を育 成 し 、民 間 投 資 を 呼 び 込 み つ つ 、ス タ ジ ア ム ・ ア リ ー ナ の 整 備等によるスポーツを通じた地域・経済の活性化に貢献し ます 来年の2018年ピョンチャン冬季大会、そして2020年 東 京 オ リ ン ピ ッ ク ・ パ ラ リ ン ピ ッ ク 競 技 大 会 に 向 け て 、日 本 選 手が多くのメダルを獲得できるよう、国立強化拠点施設を 拡充し、国際競技力の向上に取り組みます 日本で開催されるラグビーワールドカップや女子ハンド ボ ー ル 世 界 選 手 権 等 の 成 功 を 通 じ て 、オ リ ン ピ ッ ク ・ パ ラ リ ンピック・ムーブメントを広げていきます 大 会 の 場 を「 シ ョ ー ケ ー ス 」と し て 、自 動 走 行 や 水 素 社 会 な ど最先端の科学技術を世界に発信し、国内外への展開を 図 る と と も に 、サ イ バ ー セ キ ュ リ テ ィ を し っ か り 組 み 込 ん だ 安全で品質の高いICTサービスを実現します II 地方創生・農林水産・中小企業 復興 東日本大震災から6年半あまりが経過した地震・津波被災 地域の復興については、「復興期間が終了する2020年度 までに必ずやり遂げる」という強い意志をもって全力で取り 組みます 原子力災害からの復興を目指す福島については、復興期 間 後 も 継 続 し て 、国 が 前 面 に 立 っ て 中 長 期 的 、計 画 的 な 見 通しのもとに安心して帰還できるよう取り組みます いまだ約9万人の方々が避難生活を余儀なくされているた め、長期避難生活への対応のほか、新たな生活をスタート させた方々のコミュニティ再生や心のケア等にも配慮した 生活支援を行います 復興道路・復興支援道路のほか、災害公営住宅建設、防災 集団移転事業等の約9割に完了の目途がつくなど、着実に 進 捗 し て い ま す 引 き 続 き イ ン フ ラ の 整 備 に 全 力 で 取 り 組 みます 道路、鉄道、港湾等の基幹インフラ復旧を一日でも早く前倒 しできるよう着実に推進します また、常磐自動車道におい て、福島県と宮城県で混雑の見られる区間について、復興・ 創 生 期 間 内 に 4 車 線 化 を 実 現 す る と と も に 、併 せ て 、追 加 インターチェンジの整備も進めます 被災地の産業・なりわいの再生に向け、観光復興、風評払 拭を含めた農林水産業の再生と生産品の販路回復など、き め細かな再生支援に取り組みます 原発事故被災地では、帰還困難区域を除くほとんどの地域 で避難指示が解除され、帰還困難区域についても、改正福 島特措法による特定復興再生拠点整備が始まるなど、本格 的な復興がスタートしています 早く安心して帰還できるよ う、医療・介護・教育・買い物など生活面と福島イノベーショ ン・コースト構想や福島新エネ社会構想の推進、事業・営農 再開支援等により地域の産業・雇用面の環境整備に取り組 みます 福島第一原子力発電所の廃炉・汚染水対策をはじめ、中間 貯蔵施設の整備や指定廃棄物等の処理などについては、 安全を最優先に関係者の理解のもと、引き続き国が前面に 立 っ て 取 り 組 み ま す 併 せ て 、放 射 線 に 関 す る 正 し い 知 識 の普及強化による風評やいじめ対策、不当な輸入規制の撤 廃も進めます 生活の見通しと希望が持てるよう、住宅の再建や復興まち づくりの完成時期を示した「住まいの復興工程表」に沿っ て、被災者の方々に一日も早く恒久的な住宅に入っていた だけるよう引き続き取り組みます 仮設住宅での避難生活の長期化や災害公営住宅への転 居、新天地への移住等、被災者の方々の生活が多様化して いることを踏まえ、心と体の健康維持に万全を期すため、必 要な人員を確保しつつ、支援を充実強化します 2016年に発生した熊本地震により被災した地域の復旧・ 復 興 に つ い て は 、道 路 、鉄 道 、港 湾 等 の 基 幹 イ ン フ ラ の 整 備、熊本空港ターミナルビルの再建に関するコンセッショ ン方式の活用や被災地の住宅再建・宅地の復旧等に対す る支援を着実に推進します また、熊本地震の教訓を受け、 非常災害時の国による港湾の耐震強化岸壁の整備や利用 調整により、海上からの支援を円滑に進めます 大規模な災害を受けた鉄道の災害復旧を速やかに行うた め、「鉄道軌道整備法」を改正します 地方創生 地方の意欲的な取組を、情報面(地域経済分析システム (RESAS)等)、人材面(プロフェッショナル人材の活用等)、 さらには財政面(地方創生推進交付金、地方創生応援税制 (企業版ふるさと納税)等)から積極的に支援し、先駆的・成 功事例を全国展開します 地方大学の振興や地方の若者の雇用機会創出等に取り組 むため、産官学連携の下、地域の中核的な産業振興とその 専 門 人 材 育 成 等 を 行 う 優 れ た 取 組 を 支 援 し ま す ま た 、地 方における地元企業等に就職した者に対する奨学金支援 制度を促進します 地域の魅力ある自然・歴史・文化・伝統に触れつつ、ライフス テージに応じて豊かな生活を送ることができるなど、地方生 活の魅力を具体的に発信して、地方移住を促進します 「 地 域 経 済 牽 引 事 業 」の 推 進 、地 元 特 産 品 の 開 発 ・ 販 路 拡 大 へ の 支 援 、観 光 客 を 呼 び 込 む 観 光 地 域 づ く り 等 に よ る ロ ー カ ル ・ ア ベ ノ ミ ク ス の 実 現 、生 涯 活 躍 の ま ち の 推 進 、地 域 住 民等が良好な環境の形成や地域の魅力向上に取り組むエ リアマネジメントを含む自立するまちづくりの推進等を更 に進めます 出生率が向上するよう、地域の保育や医療を充実し、安心 して子供を産み、育てることができる環境づくりや働き方改 革を進めます 「第4次産業革命」(IoT・ビッグデータ・AI)の社会実装、先 端的技術による「医療・介護革命」を進め、シェアリングエコ ノミー等を活用した新しい豊かな地方のくらしを実現しま す 地域の特性を生かした地域の成長力を確保するため、地域 における空き店舗、遊休農地、古民家等の遊休資産の活用 などの取組を進めます 本社機能の地方移転・拡充を積極的に支援するとともに、 文化庁を京都に全面的に移転するほか、消費者庁、総務省 統計局等についても着実に地方移転の取組を進め、さらに ICT活用等によるすべての中央省庁の地方移転の実証実 験に取り組みます 人口減少や高齢化が著しい中山間地域等において、地域 住民が主体となった地域運営組織の形成を進めるととも に、生活サービス機能の集約・確保、集落生活圏内外との 地域公共交通ネットワークを確保することによる「小さな拠 点」の形成を推進します 地方都市の魅力ある経済・生活圏を形成するため、「コンパ ク ト + ネ ッ ト ワ ー ク 」で ま ち づ く り に 取 り 組 み 、コ ン パ ク ト シ ティの形成や地域公共交通ネットワークの再構築を通じ暮 らしやすいまちづくりを進めます 「自転車活用推進法」に基づき、国及び地方公共団体の「自 転 車 活 用 推 進 計 画 」の 策 定 を 促 進 す る と と も に 、自 転 車 通 行 空 間 の 整 備 、良 質 な 自 転 車 の 供 給 体 制 の 整 備 、コ ミ ュ ニ ティサイクルの普及促進等を通じ、自転車の活用を推進し ます 地域経済を支える建設業・運輸業・造船業等の経営基盤の 強化とともに、それを支える人材の確保・育成を推進します 「無電柱化の推進に関する法律」に基づき、電柱・電線が無 い 状 態 が 標 準 で あ る と の 認 識 の 下 、電 線 管 理 者 に よ る 地 中 化 を 推 進 す る な ど 、電 柱 ・ 電 線 の 道 路 上 に お け る 設 置 の 抑 します 農林水産業イノベーションを創出します 農林漁業者等の ニ ー ズ を 踏 ま え ロ ボ ッ ト 、I C T 、人 工 知 能( A I )な ど を 活 用 す る と と も に 、国 ・ 都 道 府 県 ・ 大 学 ・ 民 間 企 業 の「 知 」の 総 力 を結集し、現場と一体となって技術革新を進めます 「森林環境税(仮称)」の創設に向け、平成29年中に結論を 得 ま す 併 せ て 林 業 の 成 長 産 業 化 を 実 現 し ま す 意 欲 と 能 力のある経営者に森林の管理経営を集積・集約化するとと もに、市町村が森林を管理する新たな森林管理システムを 構築し、路網整備等を重点的に支援します 林業の新規就業者等多様な担い手を育成します ICT等を 活用した生産性向上の推進、CLT等新需要の拡大による国 産 材 の 利 用 拡 大 を 図 り ま す 治 山 事 業 に よ る 事 前 防 災 ・ 減 災対策を推進します 花粉症対策苗木の植替え等を進め、 花粉症ゼロ社会を実現します 国 際 環 境 の 変 化 に 対 応 す る た め 、「 広 域 浜 プ ラ ン 」 に 基 づ く 漁 船 ・ 機 器 の 導 入 、施 設 の 再 編 整 備 な ど や 、計 画 的 な 代 船 建造を進め、漁業・養殖業を持続可能な収益性の高い操業 体制へ転換し、水産日本の復活を目指します 収入安定対策や燃油・配合飼料対策を実行するほか、担い手対策、漁業の構造改革、水産物の流通・消費の拡大、海外 輸出の促進などにより、漁業の成長産業化を実現します 資源調査・資源管理の充実を図るとともに、漁場環境の保 全などを推進し、増養殖対策を講じます 外国漁船による違法操業の抑止や周辺国との国際的な資 源管理の強化などにより、日本周辺水域における資源の回 復を図ります 「 浜 プ ラ ン 」を 進 め 、国 境 監 視 な ど 多 面 的 機 能 の 発 揮 対 策 、 離島漁業再生に向けた漁業集落の活動、特定有人国境離 島地域での雇用の創出を推進し、漁港・漁村地域を活性化 します 鯨類をはじめとする水産資源の持続的活用の方針を堅持 し、本年6月に成立した捕鯨法に基づき、商業捕鯨の早期 の再開を目指します 中小企業 地域経済の主役である中小企業・小規模事業者が直面す る 、人 手 不 足 、マ ー ケ ッ ト 縮 小 な ど の 課 題 に 対 し 、起 業 の 活 性化、地域の強みや魅力を活かした商品開発や海外展開 を 含 め た 販 路 開 拓 、人 材 育 成 、人 材 投 資 の 推 進 、I C T ・ I o T 導入支援の強化など生産性向上の取組みなどを通じ、中小 企業・小規模事業者の成長、発展を促進します とりわけ小 規 模 事 業 者 に は 手 厚 い 支 援 を 行 い ま す ま た 地 域 コ ミ ュ ニ ティを支える商店街の自立を積極的に支援します 支援機関によるサポート制度や固定資産税の軽減措置等 を活用することにより、中小企業・小規模事業者の設備投 資を促進します 手続きに関しても引き続き簡素化に取り 組みます 中小企業・小規模事業者の収益力の向上と地域に根付い た価値ある事業の次世代への承継のため、承継の準備段 階から承継後まで切れ目のない支援を集中的に推進します その際、事業承継税制の様々な要件を拡充するなど、 税 制 を 含 め た 徹 底 し た 支 援 を 講 ず る と と も に 、M & A を 通 じ た事業承継の支援を進めてまいります 働き方改革で求められる対応や必要性について、中小企 業 ・ 小 規 模 事 業 者 に 対 す る 周 知 徹 底 を 図 る と と も に 、都 道 府 県 や 商 工 会 ・ 商 工 会 議 所 が 連 携 し 、働 き 方 改 革 に 取 り 組 む中小企業・小規模事業者に対するよりきめ細やかな充実 した支援を行います 地域の支援機関や専門家、よろず支援拠点などを通じて 「 も の づ く り ・ 商 業 ・ サ ー ビ ス 補 助 金 」 、「 小 規 模 事 業 者 持 続 化補助金」などの施策を地域の隅々まで行き渡らせるとと も に 、商 工 会 ・ 商 工 会 議 所 へ の「 伴 走 型 補 助 金 」な ど を 通 じ て中小企業・小規模事業者へのきめ細かな支援を行い、ローカル・アベノミクスの実現を図ります 下請取引のあり方を改善し下請企業の適正な収益を確保 するため、主要業界で策定された自主行動計画の実行を求 め て い く と と も に 、策 定 業 種 の 拡 大 を 図 り ま す ま た 、独 占 禁止法や下請代金法の運用の徹底・強化を図ることにより 不当行為の取締りを進めます 金 融 機 関 が 、 中 小 企 業 ・ 小 規 模 事 業 者 に 寄 り 添 い 、「 ひ と 手 間 か け て 育 て る 」金 融 の 機 能 を 十 分 に 発 揮 す る こ と が 重 要 であるため、借り手側からの意見も聞きながら、経営者保証 に 依 存 し な い 資 金 繰 り の 徹 底 を 図 る と と も に 、信 用 保 証 制 度の見直しなどを進めます III 安全安心 社会保障 消費税財源により社会保障制度を持続可能なものとすると と も に 、安 定 的 な 財 源 確 保 を 図 り 、子 供 か ら 、現 役 期 、高 齢 期まで生涯を通じた全世代型の社会保障を構築します 若い世代への公的支援を充実するため、「子育て安心プラ ン」を着実に実施し待機児童の解消を図るとともに、子育て 支援サービスの質の改善にも取り組みます 妊娠・出産・子 育ての切れ目ない支援を進めるとともに、児童虐待防止対 策やひとり親家庭支援を強化します 保育の受け皿については、「子育て安心プラン」を前倒し し、2020年度までの3年間で約32万人分の保育の受け皿 を整備します 国民皆保険制度を維持するとともに、病床の機能分化・連 携の推進、在宅医療の充実、地域の医療従事者確保対策を 進め、誰もが安心して受けられる医療の確保を図ります ま た、地域包括ケアシステムを強化し、住み慣れた地域で「切 れ目のない医療・介護」が受けられるよう、医療・介護サービスの体制整備を一体的に推進します データヘルスを推進し、病気や介護の予防、重度化防止対 策 を 強 化 し 、医 療 に お け る I C T 、I o T 、A I の 活 用 を 進 め る と ともに、医師、歯科医師、薬剤師、看護師等の専門職の活躍 を推進します 地域の実情に応じた介護サービスの整備や介護人材の確 保を進め、介護離職ゼロを実現するとともに、認知症の方と 家族を支援します 望まない受動喫煙をなくすため、法整備も含め受動喫煙対 策 を 徹 底 し ま す 併 せ て 、が ん の 予 防 、治 療 ・ 研 究 、患 者 の 雇用継続や療養生活の質の維持向上に取り組みます 確立された基礎年金2分の1国庫負担の下で、働き方の多 様化や就業期間の伸長を踏まえて厚生年金の適用拡大や 就労と年金受給の選択の多様化等を進め、長寿社会にお いても若い世代や低所得の方々にも安心できる年金制度 を構築します 生活保護世帯の子供の進学支援の強化など生活困窮者の 自立に向けた支援や子供の貧困対策を強化します 「自殺 総 合 対 策 大 綱 」に 基 づ き 誰 も 自 殺 に 追 い 込 ま れ る こ と の な い社会を作ります 乳幼児健診や学校検診などの健康情報の電子化、情報の連 携を進めることにより、ライフステージを通じた健康管理を 可能にし、併せてビッグデータとしての利活用を促進します 公共交通、道路、建築物等のバリアフリー化を推進し、ユニ バ ー サ ル デ ザ イ ン の ま ち づ く り を 進 め る と と も に 、バ リ ア フ リー化への国民の理解と協力を深める「心のバリアフリー」 を推進します 教育 「 教 育 は 国 家 の 基 本 」と の 考 え の も と 、安 定 財 源 を 確 保 し 、 「幼児教育振興法」の制定と幼児教育の無償化、低所得世 帯の児童生徒への支援強化、学生などへの給付型や無利 子奨学金・授業料減免の拡充を行うとともに、「卒業後拠出 金方式」を検討し、教育の機会均等を実現します 教師が子供たちと向き合う時間を増やすため、ICTによる 環境整備を進め、学校での働き方改革を行うとともに、新学 習指導要領の円滑な実施に向けて、学校の指導・事務体制 の効果的な強化やサポートスタッフの設置、部活動指導員 の普及などを行い、「チーム学校」をつくります いじめや不登校、発達障害などへの対策を強化するため、 スクールカウンセラーやソーシャルワーカー、特別支援教 育 支 援 員 な ど の 相 談 や 支 援 体 制 を 拡 充 し ま す ま た 、イ ン ターネット内での問題行動に対する取り組みを強化すると ともに、「家庭教育支援法」を制定します 高等教育の質を向上し、連携・統合・撤退などの改革構想 を明確にしつつ、国公立大、私大、高専、専修学校への支援 を強化し、高校との接続改革、社会人の学び直しなどのリカ レント教育を推進します 学校施設は、子供の学習の場であるとともに、地域社会や 防災、国民保護の拠点としての役割を果たすことから、耐震 化 の 完 了 、老 朽 化 対 策 や 空 調 設 備 、ト イ レ 改 修 な ど の 学 習 環境整備を強力に支援します 文化庁の京都移転と機能を強化します 学校における文化 芸術体験機会の確保、伝統文化などの担い手の育成支援 や文化施設の充実、文化財の保存・修理・活用の好循環を 構築していきます 2020年東京オリンピック・パラリンピック競技大会に向け て、全国各地の文化プログラムを支援し、芸術祭や食文化、 日本遺産などを国内外へ発信するとともに、メディア芸術 の 情 報 拠 点 を 整 備 し ま す 日 本 ブ ラ ン ド の 価 値 向 上 と 文 化 GDPの好循環を目指し、東京大会後のレガシー(遺産)を創 出します 「 ア ン チ ・ド ー ピ ン グ 法 」を 制 定 す る と と も に 、障 害 者 ス ポ ー ツ及び学校や社会体育施設での生涯スポーツを推進し、ス ポーツ産業などを振興します 環境 地球温暖化を食い止めるため、「パリ協定」の実施に貢献します 再生可能エネルギーの導入拡大等により、2030年度 温室効果ガス26%削減目標の達成に取り組むとともに、 2050年80%削減を目指し、経済成長につなげるための長 期戦略を策定します また、気候変動の影響を軽減する適応 策の充実強化を図るための法制度の整備を行います 「改正モントリオール議定書」への対応を含め上流から下 流までのフロン類の総合的な対策に取り組み、技術開発を 大胆に進め、世界の先頭に立ちます ライフサイクル全体での資源循環への取り組みを加速し、 食品ロス削減の取り組みを強化します マイクロプラスチッ ク等の海洋ごみの実態把握や発生抑制対策等を進めます 廃棄物処理施設の広域化・集約化、老朽化施設の更新、余 熱利用の推進等を通じ、地域の安全・安心を確保します ま た 、浄 化 槽 の 普 及 を 通 じ 、暮 ら し や す い ま ち づ く り を 目 指 し ます PM2.5の科学的知見を充実させ、関係国との連携協力や国 内対策を推進するとともに、化学物質と健康に関する調査等 を通じて国民が健康に暮らせる環境づくりに努めます ヒアリ等の外来生物対策を進めるとともに、ジビエの利用 拡 大 を 含 む 鳥 獣 被 害 対 策 を 強 化 し ま す ま た 、愛 護 動 物 の 虐待を無くし、不適切な動物取扱業者への対応を強化しま す さ ら に 、小 動 物 の 動 物 看 護 師 の 将 来 的 な 国 家 資 格 化 又 は免許制度の創設に向けた検討を行います 国土強靭化 あらゆる自然災害からかけがえのない国民の生命と財産を 守るため、「国土強靱化基本法」に基づき、事前防災・減災、 老朽化対策を強力に推進します 首都直下地震、南海トラフ地震や巨大津波に備えるため、 緊急輸送ルート、住宅・建築物、道路、堤防、鉄道、港湾等の インフラの耐震化やリダンダンシーの確保、災害に強い物 流システムの構築等により、国土の強靱化を推進します 「世界津波の日」の理念を全世界に展開させ、世界の多くの 国で共有される津波の脅威に対し、国際社会が津波の理解 と津波対策の重要性に関する理解を深め、相互に協力する ことにより、津波による犠牲者の数を減少させる取組みを 推進します 安定的・持続的な見通しをもって計画的に必要な公共投資 を行うとともに、改正品確法等に基づく取り組みや働き方改 革の推進により建設産業の担い手の確保・育成を図ります 「第4次社会資本整備重点計画」に基づき、インフラ老朽化 対策、既存ストックの活用を図りながら、中長期的な見通し を持って、ストック効果の高い事業への選択と集中を推進 します 国土強靱化に資する高速道路のミッシングリンクの解消等 に つ い て 従 来 の 事 業 評 価 に と ら わ れ る こ と な く 、国 民 に 約 束した基幹ネットワークの整備を進めます 人口減少、高齢化が進展する中、これを克服し、老朽化対策 や防災・減災対策、ストック効果を高めるアクセス道路の整 備など地方創生や国土強靭化に資する地方の道路整備を 引き続き重点的・計画的に支援します ライフラインである水道の老朽化対策や地域の医療機関等 の耐震化等の強靱化を進めます 地域包括ケアシステム等 の構築、医療・介護連携を中心とした街づくりによって、ソフ ト(人材、連携等)面でも国土強靭化を果たしていきます 上下水道の老朽化対策や耐震化等のライフラインの防災 対策、ゲリラ豪雨に備えた下水道等の排水施設の効果的な 整備を進めるとともに、豪雪地帯における除排雪や融雪に 対する支援の強化を図ります 国・地方公共団体・事業者等が個々に収集・管理している  防災・減災に資する情報の共有とICTの活用を通じ、国民 目 線 で 、迅 速 か つ 効 果 的 な 防 災 ・ 発 災 対 応 、復 旧 支 援 を 行 います 漁港施設の地震・津波対策及び長寿命化対策を進め、自然 災害に強い漁村づくり、国土強靱化を実現します 災害事象の監視体制の強化とJアラート(全国瞬時警報シ ステム)をはじめとした防災情報提供手段の多様化・高度 化を図るとともに、消防団を中核とした地域防災力の充実・ 強化に取り組み、災害対応の標準化及び防災教育訓練施 設の充実を図ります 地下シェルターの整備等の国民保護関連施策の強化に加 えて、公共・民間の既存の地下空間を活用して緊急避難場 所を確保するための新たな取組を早急に進めるとともに、 国民保護にも大きな効果を発揮する国土強靱化の取組を 加速します 治安・ テロ 2020年東京オリンピック・パラリンピック競技大会に向け、 治安関係の人的・物的基盤の拡充、外国機関との連携強化 等 を 通 じ て 、国 内 テ ロ 防 止 の 取 組 を 促 進 し つ つ 、 関 係 機 関 間の情報共有の迅速化を図るなど、国内組織のあり方の研 究 ・ 検 討 を 不 断 に 進 め 、「 世 界 一 安 全 な 国 、 日 本 」 を 実 現 し ます 国際テロの脅威の拡散を受け、「国際テロ情報収集ユニッ ト」の活動を拡大・強化しつつ、官邸を司令塔に情報収集・分 析・発信を一層推進し、在外邦人・企業・学校・公館等の安全 を確保します また、警戒・警備体制、水際対策、地域との連 携等を強化し、国内テロ防止態勢を早急に拡充します 国民が安心して利用できるサイバー空間を確保するため、 官民連携を進め、サイバーセキュリティを一層強化するとと もに、サイバー犯罪やサイバー攻撃などへの対処能力の強 化等に努めます 近年普及が進むIoT機器のセキュリティ対 策 を 強 化 す る と と も に 、保 険 制 度 な ど 新 た な 試 み を 推 進 します 高齢者が被害に遭いやすい特殊詐欺や悪質商法の被害を 防 止 す る た め 、取 締 り を 強 化 す る と と も に 、金 融 機 関 、関 係 事業者等、官民一体となった予防活動を推進します 世界一安全な道路交通を実現するため、高齢運転者による 交通事故防止対策の一層の強化、効果的な交通安全施設 等の整備、緻密で科学的な交通事故事件捜査の推進等、総 合的な交通事故抑止対策に取り組みます テロへの関与が疑われる外国人が、日本への帰化によって 日本人としてわが国に潜伏することを防止するため、より慎 重に帰化許可申請の審査を行い、関係機関との連携を強 化します 社会・生活安全・消費者 性的指向・性自認に関する広く正しい理解の増進を目的と した議員立法の制定を目指すとともに、各省庁が連携して 取 り 組 む べ き 施 策 を 推 進 し 、多 様 性 を 受 け 入 れ て い く 社 会 の実現を図ります 労働力人口が減少し、現行制度でも外国人労働者の大幅 な増加が見込まれる中で、日本人だけでは労働力が不足し 社会に深刻な悪影響が生じる分野について、外国人労働 者が適切に働ける制度を整備します わが国で生活する優秀な外国人材が、日本への帰化を希 望する場合には、その許否について速やかに判断を行う取 組を推進していきます 高速バス・貸切バス等の一層の安全性向上のため、全国規 模の迅速かつ集中的な安全強化策の実施や継続的なフォ ローアップを通じ、事故の再発防止・利用者の信頼回復を 図 る と と も に 、陸 ・ 海 ・ 空 に お け る 運 輸 安 全 の 確 保 ・ 強 化 に 全力で取り組みます 法の支配を徹底し国民の権利や国益を守るため、予防司法 機能の全国規模での充実や国際的な法的紛争対応の支援 など、国の訟務機能を強化します 安全で安心して暮らせる社会の実現に向け、「再犯防止推 進 法 」に 基 づ き「 再 犯 防 止 推 進 計 画 」を 策 定 し 、国 ・ 地 方 公 共団体・民間が一体となって再犯防止施策を強力に推進し ます わが国の良好な治安を支える保護司等更生保護に携わる民 間協力者の活動が一層充実するよう、更生保護サポートセ ンターを全ての保護司会に設置する等支援を強化します 消費者の安全で安心な暮らしを守るために、消費者行政の 強 化 を 目 指 し ま す ま た 、消 費 者 と 事 業 者 双 方 の 信 頼 関 係 を 構 築 し て い く と と も に 、自 立 し た 消 費 者 を 育 成 し 、公 正 で 持続可能な社会環境を作ります IV 国の基本 外交 国際協調主義に基づく積極的平和主義のもと、日米同盟を 基軸に、豪州、インド、ASEAN、欧州など普遍的価値を共有 する国々との連携を強化し、「自由で開かれたインド太平洋 戦略」など地球儀を俯瞰する外交を更に力強く進めていきます 北朝鮮の重大かつ差し迫った脅威に対して、制裁措置の厳 格な実施と更なる制裁の検討を行うなど国際社会と結束し て圧力を最大限に強化しつつ、関係国政府・議会及び国連 に 対 す る 連 携 や 働 き か け を 強 化 し 、核・ミ サ イ ル 開 発 の 完 全 な 放 棄 を 迫 り 、こ の よ う な 状 況 で あ る か ら こ そ 、あ ら ゆ る 手段に全力を尽くして拉致被害者全員の即時帰国を実現 します 韓国、中国、ロシアはじめ近隣諸国との関係改善を加速す るとともに、歴史認識等を巡るいわれなき非難には断固反 論 す る な ど 、わ が 国 の 名 誉 と 国 益 を 守 る た め 、戦 略 的 対 外 発信を強化します 北方領土、竹島、尖閣諸島の領土・主権 に 係 る 第 三 者 機 関 を 設 置 し 、歴 史 的 ・ 学 術 的 な 調 査 ・ 研 究 の 充 実 に 努 め 、常 設 展 示 等 も 活 用 し つ つ 、客 観 的 事 実 を 世 界に広く示します 関係国と連携して国連改革を推進し、わが国の安保理常任 理事国入りの実現に向けた取り組みを強化します また、国 際機関に対する分担金・拠出金を適切に確保し、その質を 高 め る 評 価 を 行 い 、邦 人 職 員 の 増 強 と と も に 、発 信 力 や プ レゼンスの強化に努めます 国 益 を よ り 重 視 し た 大 綱 の も と 、わ が 国 の O D A と 民 間 の 投 資を有機的に結合し、日本経済の海外進出を一層強固にし つつ、ODAの成果の評価を行うことで納税者の理解を得ら れる効果的な開発協力を推進します 自由貿易や国益に即した経済連携交渉、投資協定・租税条 約の締結を推進して諸外国の活力をわが国の成長に取り 込 み 、力 強 い 経 済 成 長 を 達 成 す る と と も に 、国 益 確 保 の 観 点からグローバルなルールの策定への貢献を推進します また、中小企業を含む日本企業及び地方自治体の海外展 開支援を強化します 多岐にわたる外交課題に取り組み、わが国の国益を確保す るため、積極的な議員外交の展開と併せて、外交実施体制 を欧米主要国並みに整備するなど、わが国の外交力を強 化します アジアを中心とした国々において、法の支配やグッドガバナ ンス(良い統治)が実現し、その国の持続的な成長に貢献 するための法制度整備支援に積極的に取り組みます 日本型司法制度の強み等を「司法外交」の重要なソフトパ ワーとし、2020年にわが国で開催される国連犯罪防止刑 事司法会議に向け、国内外の取組を戦略的に進めます アジアNo1を目指し、多数の国際仲裁事案を呼び込み、国 際紛争解決のハブになるべく、わが国の仲裁センター機能 を抜本的に強化するため必要な環境整備に取り組みます 安全保障 北朝鮮の度重なる核実験やミサイル発射、中国の急激な軍 拡や海洋進出など、わが国を取り巻く安全保障環境が激変 する中、「不戦の誓い」を堅持しつつ、国民の命や平和な暮 ら し 、領 土 ・ 領 海 ・ 領 空 を 断 固 守 り 抜 く た め 、万 全 の 態 勢 を 構築します イージスアショア等の導入を含め、わが国の弾道ミサイル 対処能力の向上や、南西地域への部隊配置等による島嶼 防衛の強化など、重大かつ差し迫った脅威や不測の事態に 対処できる態勢を整備します また、より実践的な住民避難 訓練の実施等、新たな段階に応じた国民保護の態勢を確 立します 平和安全法制により、あらゆる事態への切れ目のない対応 や邦人救出等の新任務が可能となったことを受け、態勢構 築 や 能 力 向 上 を 加 速 す る と と も に 、日 米 同 盟 や 友 好 国 と の 協力を不断に強化し、わが国の抑止力の向上を図ります 国際社会の平和と安定の確保にも引き続き積極的に貢献 します 周辺情勢の激変を受け、自衛隊の人員・装備の増強など防 衛 力 の 質 と 量 を 抜 本 的 に 拡 充 ・ 強 化 す る た め 、新 中 期 防 の 策定と現行大綱の見直しを行います 隊員の名誉や処遇の 向上にも引き続き取り組みます 防衛装備庁や防衛装備移転三原則のもと、戦略的に研究 開 発 や 友 好 国 と の 防 衛 装 備 ・ 技 術 協 力 を 推 進 し 、国 内 の 技 術的優越を確保しつつ防衛生産・技術基盤を維持・強化し ます 日米安保体制の抑止力を維持しつつ、沖縄等の基地負担 軽減の実現のため、普天間飛行場の辺野古移設や在日米 軍 再 編 を 着 実 に 進 め る と と も に 、基 地 周 辺 対 策 と し て 関 係 自治体への重点的な施策を実施します 米国政府と連携し て事件・事故防止を徹底し、日米地位協定はあるべき姿を 目指します わが国の安全保障に資する宇宙利用やサイバーセキュリ ティ対策を促進するとともに、わが国の安全に関わる対外 的な情報収集を専門的に行うため、国家の情報機能と体制 を強化します 海上保安 わが国の領土・領海の堅守に万全を期し、国民が安全・安 心に暮らすことができる平和で豊かな海を守り抜くため、海 上保安庁の海上法執行能力、海洋監視能力、海洋調査能力 の 強 化 を 図 り ま す ま た 、国 境 画 定 の 起 点 等 遠 隔 離 島 に お ける活動拠点の整備等を推進します 政治・行政改革 限られた資源を効果的に使い、最大限の成果を生むため、 既存制度の改善や見直し、規制緩和など、大胆な行政改革 を進め、合理的で活力ある行政を構築します 「 根 拠 に 基 づ く 政 策 立 案( E B P M )」の も と 、統 計 デ ー タ や 社 会科学の知見に基づき、目的や効果を明確に説明できる、 透明性を持った政策立案・予算編成に取り組みます 政治が責任を持って歳出改革に取り組み、「PDCAサイク ル 」の 視 点 か ら 、非 効 率 的 な 事 業 を 洗 い 出 し 、事 業 の 改 善 や予算の縮減、廃止、効率化などを行います ビッグデータやICTなどの利活用を推進し、マイナンバー制 度を活用した手続きの簡素化や添付書類の削減などの国 民の利便性向上と行政の効率化を行います セキュリティ対策に配意しつつ、戸籍事務にマイナンバー 制度を導入して、婚姻届等の行政手続において戸籍証明 書の添付省略を実現し、国民の利便性の向上を図ります 国家公務員の違法な再就職を根絶するため、体制強化した 再就職等監視委員会における監視を徹底し、再就職の経緯 が確認できるよう、届出・公表制度を抜本的に見直します 公務員に有為な人材を確保し、能力を生涯現役で社会に 活かすため、公務員の生涯にわたるキャリアパスや、定年 延長などを視野に入れた定数制度の見直しについて検討 します 行政システムのクラウド化・共通化の推進、IT調達の見直し を通じて大幅な行政コストの削減を行います 国民への情報公開、説明責任を全うするため、行政文書の 適正な管理に努めます 選挙権年齢が18歳以上に引き下げられたことを踏まえ、被 選挙権年齢についても引き下げの方向で検討します 対 象・適用年齢は若者団体等広く意見を聴いた上で結論を出 します また、身体に障害のある選挙人などの投票環境の 向 上 方 策 を 検 討 し ま す さ ら に 、選 挙 運 動 規 制 等 の 公 選 法 全般の見直しも進めます 道州制の導入に向けて、国民的合意を得ながら進めていき ます 導入までの間は、地域の自主自立を目指し活力が発 揮できるよう、地方公共団体間での広域的な連携の取組み の 後 押 し す る た め 、広 域 連 合 の 活 用 、道 州 制 特 区 法 の 活 用 などを検討します 婚姻時における夫婦の姓や親子関係のあり方など、家族に 関わる様々な課題について、国民的な議論を深めます 都道府県が、歴史的にも文化的にも政治的にも大きな意義 と実態を有している中で、二院制における参議院のあり方、 役割を踏まえ、憲法改正等により、3年ごとの半数改選時に 各 都 道 府 県 か ら 少 な くと も 一 人 が 選 出 さ れ る よ う 参 議 院 選 挙制度を改革します 憲法 わが党は、結党以来、「自主憲法の制定」を党是に掲げてお り、現行憲法の国民主権、基本的人権の尊重、平和主義の3 つ の 基 本 原 理 は 堅 持 し つ つ 、憲 法 改 正 を 目 指 し ま す 憲 法改正については、国民の幅広い理解を得つつ、自衛隊の明 記 、教 育 の 無 償 化・充 実 強 化 、緊 急 事 態 対 応 、参 議 院 の 合 区解消など4項目を中心に党内外の十分な議論を踏まえ、 憲法改正原案を国会で提案・発議し、国民投票を行い、初 めての憲法改正を目指します 未来に責任を持つ確かな政策で、 さらなるステージへ）\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ArrowInvalid",
          "evalue": "Column 7 named eng_text expected length 9 but got length 8",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-2274334255>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# バッチ処理で翻訳＋分類（GPU活用）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultilang_classify_climate_sentences_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Dataset → DataFrame に戻す\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m         }\n\u001b[1;32m    556\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3095\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3096\u001b[0m                 ) as pbar:\n\u001b[0;32m-> 3097\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3098\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3099\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3491\u001b[0m                                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3493\u001b[0;31m                                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3494\u001b[0m                         \u001b[0mnum_examples_progress_update\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_examples_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3495\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPBAR_REFRESH_TIME_INTERVAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    556\u001b[0m                 \u001b[0minferred_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyped_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inferred_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minferred_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_schema\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpa_writer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m         \u001b[0mpa_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mArrowInvalid\u001b[0m: Column 7 named eng_text expected length 9 but got length 8"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42aa60f9",
      "metadata": {
        "id": "42aa60f9"
      },
      "outputs": [],
      "source": [
        "df_all.to_csv(\"../data/processed/manifesto_us_japan_related_score.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BUlEX7mb5ceh"
      },
      "id": "BUlEX7mb5ceh",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9835f0dbade84163b3c9010ac5968a54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f36af6c0d36459d915c174ade6017e7",
              "IPY_MODEL_01b431dce1ee4c51a1eaa9e35aafcf4a",
              "IPY_MODEL_7851bbab49b74f7eb1459874e5443bd1"
            ],
            "layout": "IPY_MODEL_3f214e22414c4f3a967f856238f8f430"
          }
        },
        "8f36af6c0d36459d915c174ade6017e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_298f05748bb84512b70a7db418ef3748",
            "placeholder": "​",
            "style": "IPY_MODEL_51c5a0e5396b41e0a8d67be0c48117e3",
            "value": "Map:  78%"
          }
        },
        "01b431dce1ee4c51a1eaa9e35aafcf4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbe1ff43fc514ce9b58dec722e24c3cc",
            "max": 41,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc06cd84ceda465598df0aab3691e676",
            "value": 32
          }
        },
        "7851bbab49b74f7eb1459874e5443bd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6610a61cc3a84265a85f160804dd47e1",
            "placeholder": "​",
            "style": "IPY_MODEL_aa370faa9cae4e5cbf9ecef713c51843",
            "value": " 32/41 [40:10&lt;00:04,  1.98 examples/s]"
          }
        },
        "3f214e22414c4f3a967f856238f8f430": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "298f05748bb84512b70a7db418ef3748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51c5a0e5396b41e0a8d67be0c48117e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbe1ff43fc514ce9b58dec722e24c3cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc06cd84ceda465598df0aab3691e676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6610a61cc3a84265a85f160804dd47e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa370faa9cae4e5cbf9ecef713c51843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}